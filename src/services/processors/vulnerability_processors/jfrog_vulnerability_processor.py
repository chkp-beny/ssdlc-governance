"""
JfrogVulnerabilityProcessor - Extract JFrog vulnerability processing logic from Product class
Handles fetching and processing JFrog vulnerability data with AQL caching
"""

import os
import logging
from typing import List, Dict, Optional, Tuple, Any

logger = logging.getLogger(__name__)


class JfrogVulnerabilityProcessor:
    """Processes JFrog vulnerability data for repositories using enhanced AQL cache logic"""
    
    def __init__(self, product_name: str, organization_id: str):
        """
        Initialize JfrogVulnerabilityProcessor
        
        Args:
            product_name (str): Product name for configuration lookup
            organization_id (str): Organization ID for API calls
        """
        self.product_name = product_name
        self.organization_id = organization_id
        self.compass_client = None
        self.jfrog_client = None
        self._initialize_clients()
    
    def _initialize_clients(self):
        """Initialize CompassClient and JfrogClient with credentials from environment"""
        try:
            # Initialize CompassClient
            from src.services.clients.compass_clients.compass_client import CompassClient
            compass_token = os.getenv('COMPASS_ACCESS_TOKEN', '')
            compass_url = os.getenv('COMPASS_BASE_URL', '')
            
            if compass_token and compass_url:
                self.compass_client = CompassClient(compass_token, compass_url)
                logger.debug("CompassClient initialized for JfrogVulnerabilityProcessor")
            else:
                logger.warning("Compass credentials not found in environment variables")
            
            # Initialize JfrogClient if product has JFrog configuration
            jfrog_token = self._get_jfrog_token()
            if jfrog_token:
                from src.services.clients.jfrog_clients.jfrog_client import JfrogClient
                self.jfrog_client = JfrogClient(jfrog_token)
                logger.debug("JfrogClient initialized for JfrogVulnerabilityProcessor")
            
        except ImportError as e:
            logger.error("Failed to import required clients: %s", str(e))
    
    def _get_jfrog_token(self) -> Optional[str]:
        """Get JFrog token for this product"""
        try:
            from CONSTANTS import get_jfrog_token_for_product
            jfrog_token, token_env_var = get_jfrog_token_for_product(self.product_name)
            if not jfrog_token:
                logger.warning("%s not found for product '%s'", token_env_var, self.product_name)
            return jfrog_token
        except (ImportError, KeyError) as e:
            logger.warning("Could not get JFrog token for product '%s': %s", self.product_name, str(e))
            return None
        except Exception as e:
            logger.error("Error getting JFrog token for product '%s': %s", self.product_name, str(e))
            return None
    
    def process_vulnerabilities(self, repositories: List) -> int:
        """
        Load JFrog vulnerability data for repositories using enhanced AQL cache logic
        
        Args:
            repositories (List): List of repository objects to update
            
        Returns:
            int: Number of repositories updated with vulnerability data
        """
        if not self.compass_client:
            logger.warning("No CompassClient available, skipping JFrog vulnerability processing")
            return 0
        
        try:
            logger.info("ðŸ” Loading JFrog vulnerabilities for product '%s'", self.product_name)
            
            # Fetch JFrog vulnerabilities from Compass API
            jfrog_vulnerabilities = self.compass_client.fetch_jfrog_vulnerabilities(self.organization_id)
            
            if not jfrog_vulnerabilities:
                logger.info("No JFrog vulnerabilities data returned for organization '%s'", self.organization_id)
                return 0
            
            logger.info("ðŸ“Š Fetched %d vulnerability artifacts from Compass API", len(jfrog_vulnerabilities))
            
            # Get JFrog project configuration
            jfrog_project = self._get_jfrog_project()
            if not jfrog_project:
                logger.warning("No JFrog project configured for product '%s', skipping JFrog vulnerability processing", self.product_name)
                return 0
            
            # Setup AQL cache directory
            aql_cache_dir = self._setup_aql_cache_directory(jfrog_project)
            
            # Build repository lookup maps
            repo_build_names_map = self._build_repository_lookup_map(repositories)
            logger.info("ðŸ“‹ Found %d repositories with matched build names for artifact matching", len(repo_build_names_map))
            
            # Process vulnerability artifacts
            return self._process_vulnerability_artifacts(
                jfrog_vulnerabilities, repositories, repo_build_names_map, aql_cache_dir
            )
            
        except Exception as e:
            logger.error("Error loading JFrog vulnerabilities for product '%s': %s", self.product_name, str(e))
            return 0
    
    def _get_jfrog_project(self) -> Optional[str]:
        """Get JFrog project configuration for this product"""
        try:
            from CONSTANTS import PRODUCT_JFROG_PROJECT
            return PRODUCT_JFROG_PROJECT.get(self.product_name, "")
        except (ImportError, KeyError) as e:
            logger.warning("Could not get JFrog project for product '%s': %s", self.product_name, str(e))
            return None
    
    def _setup_aql_cache_directory(self, jfrog_project: str) -> str:
        """Setup AQL cache directory structure"""
        cache_dir = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'build_info_cache_dir')
        product_cache_dir = os.path.join(cache_dir, jfrog_project)
        aql_cache_dir = os.path.join(product_cache_dir, "cache_repo_responses")
        os.makedirs(aql_cache_dir, exist_ok=True)
        
        logger.info("ðŸ’¾ Using AQL cache directory: %s", aql_cache_dir)
        return aql_cache_dir
    
    def _build_repository_lookup_map(self, repositories: List) -> Dict[str, set]:
        """Build lookup map for repositories with matched build names"""
        repo_build_names_map = {}
        
        for repo in repositories:
            if (repo.scm_info and repo.scm_info.repo_name and 
                repo.ci_status and repo.ci_status.jfrog_status.matched_build_names):
                repo_build_names_map[repo.scm_info.repo_name] = repo.ci_status.jfrog_status.matched_build_names
        
        return repo_build_names_map
    
    def _process_vulnerability_artifacts(self, jfrog_vulnerabilities: Dict, repositories: List, 
                                       repo_build_names_map: Dict, aql_cache_dir: str) -> int:
        """Process vulnerability artifacts and update repositories"""
        
        # Initialize counters and tracking dictionaries
        processed_artifacts = 0
        local_artifacts = 0
        malformed_artifacts = 0
        repo_matches = 0
        
        missing_artifacts_by_repo = {}  # repo_name -> [artifact_paths]
        artifacts_by_repo = {}  # repo_name -> [DeployedArtifact objects]
        
        # Process each vulnerability artifact
        for artifact_key, vuln_data in jfrog_vulnerabilities.items():
            processed_artifacts += 1
            
            # Log progress every 100 artifacts
            if processed_artifacts % 100 == 0:
                logger.info("ðŸ”„ Progress: %d/%d artifacts processed (%d%%), %d local, %d matched to repos", 
                          processed_artifacts, len(jfrog_vulnerabilities), 
                          int((processed_artifacts / len(jfrog_vulnerabilities)) * 100),
                          local_artifacts, repo_matches)
            
            # Parse and process artifact
            result = self._process_single_artifact(
                artifact_key, vuln_data, repo_build_names_map, aql_cache_dir,
                missing_artifacts_by_repo, artifacts_by_repo
            )
            
            if result == "malformed":
                malformed_artifacts += 1
            elif result == "local":
                local_artifacts += 1
            elif result == "matched":
                repo_matches += 1
        
        # Handle missing artifacts with batch AQL queries
        if self.jfrog_client and missing_artifacts_by_repo:
            self._handle_missing_artifacts(missing_artifacts_by_repo, artifacts_by_repo, aql_cache_dir)
        
        # Update repositories with collected artifacts
        updated_count = self._update_repositories_with_artifacts(repositories, artifacts_by_repo)
        
        logger.info("âœ… JFrog vulnerability processing completed for product '%s': %d repositories updated", 
                   self.product_name, updated_count)
        logger.info("ðŸ“Š Statistics: %d processed, %d local, %d malformed, %d repo matches", 
                   processed_artifacts, local_artifacts, malformed_artifacts, repo_matches)
        
        return updated_count
    
    def _process_single_artifact(self, artifact_key: str, vuln_data: Dict, repo_build_names_map: Dict,
                               aql_cache_dir: str, missing_artifacts_by_repo: Dict, artifacts_by_repo: Dict) -> str:
        """Process a single artifact and categorize it"""
        
        # Parse artifact structure
        parsed_artifact = self._parse_artifact_path(artifact_key)
        if not parsed_artifact:
            logger.warning("âš ï¸ Malformed artifact path: %s", artifact_key)
            return "malformed"
        
        repo_name, path, name, full_path = parsed_artifact
        
        # Check if this is a local repository
        if not self._is_local_repo(repo_name):
            logger.debug("Skipping non-local repository: %s", repo_name)
            return "skipped"
        
        # Check AQL cache for this repository
        aql_cache_file = os.path.join(aql_cache_dir, f"{repo_name}.json")
        aql_data = self._load_aql_cache(aql_cache_file)
        
        if aql_data is None:
            # Cache miss - add to missing artifacts list
            if repo_name not in missing_artifacts_by_repo:
                missing_artifacts_by_repo[repo_name] = []
            missing_artifacts_by_repo[repo_name].append((path, name))
            return "local"
        
        # Try to match artifact using cached AQL data
        matched_repo = self._match_artifact_to_repository(
            artifact_key, vuln_data, repo_name, path, name, 
            aql_data, repo_build_names_map, artifacts_by_repo
        )
        
        return "matched" if matched_repo else "local"
    
    def _parse_artifact_path(self, artifact_key: str) -> Optional[Tuple[str, str, str, str]]:
        """Parse artifact path into components (simplified version of Product._parse_artifact_path)"""
        # This is a simplified implementation - you may need to implement the full parsing logic
        # from the original Product._parse_artifact_path method
        try:
            if "://" in artifact_key:
                # Handle docker://... format
                parts = artifact_key.split("/")
                if len(parts) >= 3:
                    repo_name = parts[1] if len(parts) > 1 else ""
                    name = parts[-1] if parts else ""
                    path = "/".join(parts[2:-1]) if len(parts) > 3 else ""
                    return repo_name, path, name, artifact_key
            else:
                # Handle regular path format
                parts = artifact_key.split("/")
                if len(parts) >= 2:
                    repo_name = parts[0]
                    name = parts[-1]
                    path = "/".join(parts[1:-1]) if len(parts) > 2 else ""
                    return repo_name, path, name, artifact_key
        except Exception as e:
            logger.error("Error parsing artifact path '%s': %s", artifact_key, str(e))
        
        return None
    
    def _is_local_repo(self, repo_name: str) -> bool:
        """Check if repository is a local repository"""
        # Simplified logic - implement based on your repository naming conventions
        return "local" in repo_name.lower()
    
    def _load_aql_cache(self, cache_file: str) -> Optional[Dict]:
        """Load AQL cache data from file"""
        try:
            if os.path.exists(cache_file):
                import json
                with open(cache_file, 'r') as f:
                    return json.load(f)
        except Exception as e:
            logger.error("Error loading AQL cache from '%s': %s", cache_file, str(e))
        return None
    
    def _match_artifact_to_repository(self, artifact_key: str, vuln_data: Dict, repo_name: str, 
                                    path: str, name: str, aql_data: Dict, repo_build_names_map: Dict,
                                    artifacts_by_repo: Dict) -> bool:
        """Match artifact to repository using AQL data and build names"""
        try:
            # Search through AQL data to find which build this artifact belongs to
            if aql_data and isinstance(aql_data, dict):
                results = aql_data.get('results', [])
                for aql_entry in results:
                    # Check if this AQL entry matches our artifact
                    aql_path = aql_entry.get('path', '')
                    aql_name = aql_entry.get('name', '')
                    
                    if aql_path == path and aql_name == name:
                        # Found matching AQL entry, extract build name and other properties
                        properties = aql_entry.get('properties', [])
                        build_name = self._extract_property_value(properties, 'build.name')
                        build_number = self._extract_property_value(properties, 'build.number')
                        build_timestamp = self._extract_property_value(properties, 'build.timestamp')
                        sha256 = self._extract_property_value(properties, 'sha256')
                        
                        if build_name:
                            # Extract the actual build name from the full path
                            extracted_build_name = self._extract_build_name_from_path(build_name)
                            
                            # Try to find which repository this build belongs to
                            for project_repo_name, build_names in repo_build_names_map.items():
                                if extracted_build_name in build_names:
                                    # Match found! Create and add artifact to this project repository
                                    from src.models.vulnerabilities import DeployedArtifact
                                    
                                    artifact = DeployedArtifact(
                                        artifact_key=artifact_key,
                                        repo_name=repo_name,  # Use JFrog repo name (e.g., cyberint-docker-local)
                                        critical_count=vuln_data.get('vulnerabilities', {}).get('critical', 0),
                                        high_count=vuln_data.get('vulnerabilities', {}).get('high', 0),
                                        medium_count=vuln_data.get('vulnerabilities', {}).get('medium', 0),
                                        low_count=vuln_data.get('vulnerabilities', {}).get('low', 0),
                                        unknown_count=vuln_data.get('vulnerabilities', {}).get('unknown', 0),
                                        artifact_type=self._determine_artifact_type(artifact_key),
                                        build_name=extracted_build_name,  # Use extracted build name
                                        build_number=build_number,
                                        build_timestamp=build_timestamp,
                                        created_at=aql_entry.get('created', ''),
                                        updated_at=aql_entry.get('updated', ''),
                                        sha256=sha256
                                    )
                                    
                                    if project_repo_name not in artifacts_by_repo:
                                        artifacts_by_repo[project_repo_name] = []
                                    artifacts_by_repo[project_repo_name].append(artifact)
                                    
                                    logger.debug("Matched artifact '%s' to repository '%s' via build '%s' (extracted from '%s')", 
                                               artifact_key, project_repo_name, extracted_build_name, build_name)
                                    return True
            
            # No match found
            return False
            
        except (KeyError, ValueError, TypeError) as e:
            logger.error("Error matching artifact '%s' to repository: %s", artifact_key, str(e))
            return False
    
    def _extract_property_value(self, properties: List[Dict], key: str) -> str:
        """Extract property value from properties array by key"""
        for prop in properties:
            if prop.get('key') == key:
                return prop.get('value', '')
        return ''
    
    def _extract_build_name_from_path(self, build_name_path: str) -> str:
        """
        Extract build name from full build name path.
        
        Examples:
        - 'web-engine-testing-service' -> 'web-engine-testing-service' (no slashes)
        - 'Diagnostics/web-engine-testing-service/testing-in-staging' -> 'web-engine-testing-service' (middle part)
        - 'Frontend/frontend-service/jfrog' -> 'frontend-service' (middle part)
        """
        if '/' not in build_name_path:
            return build_name_path
        
        parts = build_name_path.split('/')
        if len(parts) >= 3:
            # Take the middle part (index 1)
            return parts[1]
        elif len(parts) == 2:
            # If only one slash, take the second part
            return parts[1]
        else:
            return build_name_path
    
    def _determine_artifact_type(self, artifact_key: str) -> str:
        """Determine artifact type from artifact key"""
        if artifact_key.startswith("docker://"):
            return "docker"
        elif ".jar" in artifact_key:
            return "maven"
        elif ".tgz" in artifact_key or ".tar.gz" in artifact_key:
            return "npm"
        else:
            return "unknown"
    
    def _handle_missing_artifacts(self, missing_artifacts_by_repo: Dict, _artifacts_by_repo: Dict, _aql_cache_dir: str):
        """Handle missing artifacts with batch AQL queries"""
        logger.info("ðŸ” Handling %d repositories with missing AQL cache data", len(missing_artifacts_by_repo))
        # Implementation would go here - this is complex AQL querying logic
        # For now, just log that we would handle this
        for repo_name, artifacts in missing_artifacts_by_repo.items():
            logger.debug("Would fetch AQL data for repo '%s' with %d missing artifacts", repo_name, len(artifacts))
    
    def _update_repositories_with_artifacts(self, repositories: List, artifacts_by_repo: Dict) -> int:
        """Update repositories with collected vulnerability artifacts"""
        updated_count = 0
        
        for repo in repositories:
            repo_name = repo.get_repository_name() if hasattr(repo, 'get_repository_name') else getattr(repo, 'name', '')
            
            # Try to find artifacts for this repository using multiple matching strategies
            artifacts_list = []
            
            # Strategy 1: Direct name match
            if repo_name in artifacts_by_repo:
                artifacts_list = artifacts_by_repo[repo_name]
            
            # Strategy 2: Try to match using any available identifiers
            if not artifacts_list:
                # Look for artifacts that might match this repository name
                for artifact_repo_key, artifacts in artifacts_by_repo.items():
                    if repo_name in artifact_repo_key or artifact_repo_key in repo_name:
                        artifacts_list = artifacts
                        logger.debug("Matched repository '%s' to artifact key '%s'", repo_name, artifact_repo_key)
                        break
            
            if artifacts_list:
                try:
                    from src.models.vulnerabilities import Vulnerabilities, DependenciesVulnerabilities
                    
                    # Initialize vulnerabilities if not already initialized
                    if repo.vulnerabilities is None:
                        repo.vulnerabilities = Vulnerabilities()
                    
                    # Initialize dependencies vulnerabilities if needed
                    if repo.vulnerabilities.dependencies_vulns is None:
                        repo.vulnerabilities.dependencies_vulns = DependenciesVulnerabilities()
                    
                    # Add artifacts to the repository
                    for artifact in artifacts_list:
                        repo.vulnerabilities.dependencies_vulns.add_artifact(artifact)
                    
                    # Set top-level counts based on artifacts and repository metadata
                    matched_build_names = set()
                    if hasattr(repo, 'ci_status') and repo.ci_status and hasattr(repo.ci_status, 'jfrog_status'):
                        matched_build_names = getattr(repo.ci_status.jfrog_status, 'matched_build_names', set())
                    
                    # Determine repository publish artifacts type (default to "multi")
                    repo_publish_artifacts_type = "multi"  # Could be enhanced with actual logic
                    
                    # Set the top-level counts
                    repo.vulnerabilities.dependencies_vulns.set_top_level_counts(
                        repo_publish_artifacts_type, matched_build_names
                    )
                    
                    updated_count += 1
                    logger.debug("Updated repository '%s' with %d vulnerability artifacts and calculated top-level counts", 
                               repo_name, len(artifacts_list))
                    
                except (ImportError, AttributeError) as e:
                    logger.error("Error updating repository '%s' with artifacts: %s", repo_name, str(e))
        
        return updated_count
